#!/usr/bin/env python3  
# -*- coding: utf-8 -*-
"""
æ°´åŠ¡æ•°æ®è·å–è„šæœ¬ - æœ€ç»ˆç‰ˆæœ¬
åŸºäºæµè§ˆå™¨å¼€å‘è€…å·¥å…·å‘ç°çš„çœŸå®APIç«¯ç‚¹
"""

import requests
import re
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin


class FinalWaterDataScraper:
    def __init__(self):
        """åˆå§‹åŒ–HTTPçˆ¬è™«"""
        self.session = requests.Session()
        
        # ç³»ç»Ÿç™»å½•ä¿¡æ¯
        self.base_url = "http://axwater.dmas.cn"
        self.login_url = f"{self.base_url}/Login.aspx"
        self.username = "13509288500"
        self.password = "288500"
        
        # çœŸå®çš„APIç«¯ç‚¹ï¼ˆä»å¼€å‘è€…å·¥å…·è·å–ï¼‰
        self.api_url = f"{self.base_url}/reports/ashx/getRptWaterYield.ashx"
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Connection': 'keep-alive',
            'Referer': f'{self.base_url}/reports/FluxRpt.aspx'
        })
        
        # ç›®æ ‡æ°´è¡¨IDï¼ˆæŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„æ­£ç¡®æ ¼å¼ï¼‰
        self.target_meters = [
            '2501200108',
            '1261181000263',    # ä¿®æ­£äº†ä½æ•° 
            '1262330402331',    # ä¿®æ­£äº†ä½æ•°
            '2520005',
            '2520006',
            '1261181000300'     # ä¿®æ­£äº†ä½æ•°
        ]
        
        # ç¦ç”¨SSLéªŒè¯è­¦å‘Š
        requests.packages.urllib3.disable_warnings()
    
    def login(self):
        """æ‰§è¡Œç™»å½•"""
        print("å¼€å§‹ç™»å½•ç³»ç»Ÿ...")
        
        try:
            # è·å–ç™»å½•é¡µé¢
            response = self.session.get(self.login_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            form = soup.find('form', {'method': 'post'}) or soup.find('form')
            
            if not form:
                print("âŒ æœªæ‰¾åˆ°ç™»å½•è¡¨å•")
                return False
            
            # æå–è¡¨å•æ•°æ®
            form_data = {}
            inputs = form.find_all('input')
            for input_elem in inputs:
                name = input_elem.get('name')
                value = input_elem.get('value', '')
                if name:
                    form_data[name] = value
            
            # è®¾ç½®ç™»å½•å‡­æ®
            username_field = None
            password_field = None
            
            for input_elem in inputs:
                name = input_elem.get('name', '')
                input_type = input_elem.get('type', 'text')
                
                if 'user' in name.lower():
                    username_field = name
                elif input_type == 'password':
                    password_field = name
            
            if username_field and password_field:
                form_data[username_field] = self.username
                form_data[password_field] = self.password
                
                print("æ­£åœ¨å‘é€ç™»å½•è¯·æ±‚...")
                login_response = self.session.post(
                    self.login_url,
                    data=form_data,
                    timeout=15,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ
                if "Login.aspx" not in login_response.url or "ThinkWater" in login_response.text:
                    print("âœ… ç™»å½•æˆåŠŸï¼")
                    
                    # ä¿å­˜é‡è¦çš„Cookieä¿¡æ¯
                    cookies_info = []
                    for cookie in self.session.cookies:
                        cookies_info.append(f"{cookie.name}={cookie.value}")
                    print(f"ğŸ“ ä¿å­˜äº† {len(self.session.cookies)} ä¸ªCookie")
                    
                    return True
                else:
                    print("âŒ ç™»å½•å¤±è´¥")
                    return False
            else:
                print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·åæˆ–å¯†ç å­—æ®µ")
                return False
                
        except Exception as e:
            print(f"ç™»å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def setup_session_state(self):
        """è®¾ç½®æ­£ç¡®çš„ä¼šè¯çŠ¶æ€"""
        print("æ­£åœ¨è®¾ç½®ä¼šè¯çŠ¶æ€...")
        
        try:
            # 1. è®¿é—®æŠ¥è¡¨é¡µé¢å»ºç«‹ä¼šè¯çŠ¶æ€
            report_page_url = f"{self.base_url}/reports/FluxRpt.aspx"
            print(f"è®¿é—®æŠ¥è¡¨é¡µé¢: {report_page_url}")
            
            response = self.session.get(report_page_url, timeout=10)
            if response.status_code == 200:
                print("âœ… æˆåŠŸè®¿é—®æŠ¥è¡¨é¡µé¢")
                
                # è§£æé¡µé¢è·å–å¿…è¦çš„çŠ¶æ€ä¿¡æ¯
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è·å–ViewStateç­‰ASP.NETçŠ¶æ€
                viewstate = soup.find('input', {'name': '__VIEWSTATE'})
                eventvalidation = soup.find('input', {'name': '__EVENTVALIDATION'})
                
                if viewstate and eventvalidation:
                    self.viewstate = viewstate.get('value', '')
                    self.eventvalidation = eventvalidation.get('value', '')
                    print("âœ… è·å–åˆ°ASP.NETçŠ¶æ€ä¿¡æ¯")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°ASP.NETçŠ¶æ€ä¿¡æ¯")
                
                return True
            else:
                print(f"âŒ è®¿é—®æŠ¥è¡¨é¡µé¢å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"è®¾ç½®ä¼šè¯çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def get_water_yield_data(self):
        """ä½¿ç”¨çœŸå®APIè·å–æ°´è¡¨äº§æ°´é‡æ•°æ®"""
        print("æ­£åœ¨è°ƒç”¨çœŸå®APIè·å–æ°´è¡¨æ•°æ®...")
        
        try:
            # å°è¯•å¤šä¸ªæ—¥æœŸèŒƒå›´
            date_ranges = [
                ('2025-07-24', '2025-07-31'),  # åŸå§‹æ—¥æœŸ
                ('2024-07-24', '2024-07-31'),  # å»å¹´åŒæœŸ
                ('2024-12-01', '2024-12-07'),  # å»å¹´12æœˆ
                ('2024-11-01', '2024-11-07'),  # å»å¹´11æœˆ
            ]
            
            for start_date, end_date in date_ranges:
                print(f"\nğŸ”„ å°è¯•æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
                
                # æ„é€ APIè¯·æ±‚å‚æ•°ï¼ˆæŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„æ ¼å¼ï¼Œæ¯ä¸ªIDç”¨å•å¼•å·åŒ…å›´ï¼‰
                formatted_node_ids = "'" + "','".join(self.target_meters) + "'"
                api_params = {
                    'nodeId': formatted_node_ids,            # æ ¼å¼: '2501200108','1261181000263',...
                    'startDate': start_date,                 # å¼€å§‹æ—¥æœŸ
                    'endDate': end_date,                     # ç»“æŸæ—¥æœŸ  
                    'meterType': '-1',                       # è®¡é‡ç±»å‹ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨ï¼‰
                    'statisticsType': 'flux',                # ç»Ÿè®¡ç±»å‹ï¼ˆæµé‡ï¼‰
                    'type': 'dayRpt'                         # æŠ¥è¡¨ç±»å‹ï¼ˆæ—¥æŠ¥è¡¨ï¼‰
                }
                
                if self._try_api_request(api_params):
                    return True
            
            return False
                
        except Exception as e:
            print(f"APIè¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _try_api_request(self, api_params):
        """å°è¯•å•æ¬¡APIè¯·æ±‚"""
        try:
            print("APIè¯·æ±‚å‚æ•°:")
            for key, value in api_params.items():
                print(f"  {key}: {value}")
            
            print(f"APIç«¯ç‚¹: {self.api_url}")
            
            # æ›´æ–°è¯·æ±‚å¤´ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯
            headers = {
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                'X-Requested-With': 'XMLHttpRequest',
                'Connection': 'keep-alive',
                'Referer': f'{self.base_url}/reports/FluxRpt.aspx',
                'Origin': self.base_url,
                'Host': 'axwater.dmas.cn',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
            }
            
            # å‘é€APIè¯·æ±‚
            response = self.session.post(
                self.api_url,
                data=api_params,
                headers=headers,
                timeout=15
            )
            
            print(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹ç±»å‹: {response.headers.get('content-type', 'æœªçŸ¥')}")
            
            if response.status_code == 200:
                return self.parse_api_response(response)
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text[:500]}")
                return False
                
        except Exception as e:
            print(f"APIè¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def parse_api_response(self, response):
        """è§£æAPIå“åº”æ•°æ®"""
        try:
            content = response.text.strip()
            print(f"å“åº”æ•°æ®é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå“åº”
            if not content:
                print("âŒ å“åº”å†…å®¹ä¸ºç©º")
                return False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç™»å½•è¶…æ—¶
            if 'ç™»å½•è¶…æ—¶' in content or 'Login.aspx' in content:
                print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•è¶…æ—¶")
                return False
            
            # å°è¯•è§£æJSON
            try:
                data = response.json()
                print("âœ… æˆåŠŸè§£æJSONæ•°æ®:")
                self.display_water_data(data)
                return True
            except json.JSONDecodeError:
                pass
            
            # æ£€æŸ¥HTMLè¡¨æ ¼
            if '<table' in content or '<tr' in content:
                soup = BeautifulSoup(content, 'html.parser')
                tables = soup.find_all('table')
                
                if tables:
                    print("âœ… å‘ç°HTMLè¡¨æ ¼æ•°æ®:")
                    self.extract_table_data(tables)
                    return True
            
            # æ˜¾ç¤ºåŸå§‹å“åº”å†…å®¹
            print("ğŸ“„ åŸå§‹å“åº”å†…å®¹:")
            print("="*80)
            print(content)
            print("="*80)
            
            # å¦‚æœå†…å®¹åŒ…å«æ°´è¡¨ç›¸å…³å­—æ®µï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆæ•°æ®
            if any(keyword in content.lower() for keyword in 
                   ['æ°´è¡¨', 'meter', 'flux', 'æµé‡', 'ç”¨æ°´é‡', 'æŠ„è¡¨', 'æ•°æ®']):
                print("âœ… æ£€æµ‹åˆ°æ°´è¡¨ç›¸å…³æ•°æ®")
                return True
            
            return False
            
        except Exception as e:
            print(f"è§£æAPIå“åº”å¤±è´¥: {e}")
            return False
    
    def display_water_data(self, data):
        """æ˜¾ç¤ºæ°´è¡¨æ•°æ®"""
        try:
            print("\n" + "="*80)
            print("ğŸ“Š æ°´è¡¨æ•°æ®è·å–ç»“æœï¼š")  
            print("="*80)
            
            if isinstance(data, dict):
                # æ£€æŸ¥å¸¸è§çš„æ•°æ®ç»“æ„
                if 'rows' in data and data['rows']:
                    rows = data['rows']
                    print(f"æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
                    
                    if isinstance(rows[0], dict):
                        headers = list(rows[0].keys())
                        print("è¡¨å¤´:", " | ".join(headers))
                        print("-" * 80)
                        
                        for i, row in enumerate(rows):
                            row_data = [str(row.get(h, '')) for h in headers]
                            print(f"ç¬¬{i+1}è¡Œ:", " | ".join(row_data))
                
                elif 'data' in data:
                    print("æ•°æ®å†…å®¹:")
                    if isinstance(data['data'], list):
                        for i, item in enumerate(data['data']):
                            print(f"é¡¹ç›® {i+1}: {item}")
                    else:
                        print(data['data'])
                
                else:
                    # æ˜¾ç¤ºå®Œæ•´JSONç»“æ„
                    print("å®Œæ•´æ•°æ®ç»“æ„:")
                    print(json.dumps(data, indent=2, ensure_ascii=False))
            
            elif isinstance(data, list):
                print(f"æ‰¾åˆ° {len(data)} é¡¹æ•°æ®:")
                for i, item in enumerate(data):
                    print(f"é¡¹ç›® {i+1}: {item}")
            
            print("="*80)
            
        except Exception as e:
            print(f"æ˜¾ç¤ºæ°´è¡¨æ•°æ®å¤±è´¥: {e}")
    
    def extract_table_data(self, tables):
        """æå–HTMLè¡¨æ ¼æ•°æ®"""
        try:
            print("\n" + "="*80)
            print("ğŸ“Š HTMLè¡¨æ ¼æ•°æ®ï¼š")
            print("="*80)
            
            for table_idx, table in enumerate(tables):
                rows = table.find_all('tr')
                if len(rows) > 0:
                    print(f"\nè¡¨æ ¼ {table_idx + 1} (å…± {len(rows)} è¡Œ):")
                    
                    for row_idx, row in enumerate(rows):
                        cells = row.find_all(['td', 'th'])
                        if cells:
                            row_data = [cell.get_text(strip=True) for cell in cells]
                            if any(row_data):  # åªæ˜¾ç¤ºéç©ºè¡Œ
                                if row_idx == 0:
                                    print("è¡¨å¤´:", " | ".join(row_data))
                                    print("-" * 60)
                                else:
                                    print(f"ç¬¬{row_idx}è¡Œ:", " | ".join(row_data))
            
            print("="*80)
            
        except Exception as e:
            print(f"æå–è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
    
    def test_alternative_endpoints(self):
        """æµ‹è¯•å…¶ä»–å¯èƒ½çš„APIç«¯ç‚¹"""
        print("æ­£åœ¨æµ‹è¯•å…¶ä»–å¯èƒ½çš„APIç«¯ç‚¹...")
        
        alternative_endpoints = [
            f"{self.base_url}/reports/ashx/getWaterData.ashx",
            f"{self.base_url}/reports/ashx/FluxReport.ashx", 
            f"{self.base_url}/Handler/WaterYield.ashx",
            f"{self.base_url}/ajax/getRptData.ashx"
        ]
        
        # ä½¿ç”¨æ­£ç¡®çš„nodeIdæ ¼å¼
        formatted_node_ids = "'" + "','".join(self.target_meters) + "'"
        api_params = {
            'nodeId': formatted_node_ids,
            'startDate': '2025-07-24',
            'endDate': '2025-07-31',
            'meterType': '-1',
            'statisticsType': 'flux',
            'type': 'dayRpt'
        }
        
        for endpoint in alternative_endpoints:
            try:
                print(f"\nğŸ”„ æµ‹è¯•ç«¯ç‚¹: {endpoint}")
                response = self.session.post(endpoint, data=api_params, timeout=10)
                
                if response.status_code == 200 and response.text.strip():
                    print(f"âœ… ç«¯ç‚¹å“åº”æˆåŠŸ")
                    if self.parse_api_response(response):
                        return True
                else:
                    print(f"âŒ ç«¯ç‚¹æ— å“åº” (çŠ¶æ€ç : {response.status_code})")
                    
            except Exception as e:
                print(f"âŒ ç«¯ç‚¹æµ‹è¯•å¤±è´¥: {e}")
        
        return False
    
    def get_water_data(self):
        """å®Œæ•´çš„æ•°æ®è·å–æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æœ€ç»ˆç‰ˆæ°´åŠ¡æ•°æ®è·å–æµç¨‹")
        print("="*60)
        
        try:
            # 1. ç™»å½•
            if not self.login():
                print("âŒ ç™»å½•å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return False
            
            # 2. è®¾ç½®ä¼šè¯çŠ¶æ€
            if not self.setup_session_state():
                print("âŒ è®¾ç½®ä¼šè¯çŠ¶æ€å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return False
            
            # 3. ä½¿ç”¨çœŸå®APIè·å–æ•°æ®
            if self.get_water_yield_data():
                print("\nğŸ‰ æˆåŠŸè·å–æ°´åŠ¡æ•°æ®ï¼")
                return True
            
            # 4. å¦‚æœä¸»APIå¤±è´¥ï¼Œå°è¯•å…¶ä»–ç«¯ç‚¹
            print("\nâš ï¸  ä¸»APIæ— æ•°æ®ï¼Œå°è¯•å…¶ä»–ç«¯ç‚¹...")
            if self.test_alternative_endpoints():
                print("\nğŸ‰ é€šè¿‡å¤‡ç”¨ç«¯ç‚¹è·å–åˆ°æ•°æ®ï¼")
                return True
            
            print("\nâŒ æ‰€æœ‰APIç«¯ç‚¹éƒ½æ— æ³•è·å–æ•°æ®")
            return False
            
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–æµç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    scraper = FinalWaterDataScraper()
    
    try:
        if scraper.get_water_data():
            print("\nâœ… æœ€ç»ˆç‰ˆæ•°æ®è·å–æˆåŠŸï¼")
        else:
            print("\nâŒ æœ€ç»ˆç‰ˆæ•°æ®è·å–å¤±è´¥ï¼")
            
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()