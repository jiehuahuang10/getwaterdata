#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ°´åŠ¡æ•°æ®è·å–è„šæœ¬ - ä½¿ç”¨å®é™…æ—¥æœŸå’Œæ›´æ™ºèƒ½çš„å‚æ•°æ¢æµ‹
"""

import requests
import re
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta


class SmartWaterDataScraper:
    def __init__(self):
        """åˆå§‹åŒ–HTTPçˆ¬è™«"""
        self.session = requests.Session()
        
        # ç³»ç»Ÿç™»å½•ä¿¡æ¯
        self.base_url = "http://axwater.dmas.cn"
        self.login_url = f"{self.base_url}/Login.aspx"
        self.username = "13509288500"
        self.password = "288500"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # è®¡ç®—å®é™…çš„æ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨è¿‡å»çš„æ—¥æœŸï¼Œå¯èƒ½æœ‰æ•°æ®ï¼‰
        today = datetime.now()
        end_date = today - timedelta(days=30)  # 30å¤©å‰ï¼ˆæ›´å¯èƒ½æœ‰å†å²æ•°æ®ï¼‰
        start_date = end_date - timedelta(days=6)  # å†å¾€å‰7å¤©
        
        self.start_date = start_date.strftime('%Y-%m-%d')
        self.end_date = end_date.strftime('%Y-%m-%d')
        
        print(f"ğŸ“… ä½¿ç”¨æ—¥æœŸèŒƒå›´: {self.start_date} åˆ° {self.end_date}")
        
        # ç¦ç”¨SSLéªŒè¯è­¦å‘Š
        requests.packages.urllib3.disable_warnings()
    
    def login(self):
        """æ‰§è¡Œç™»å½•"""
        print("å¼€å§‹ç™»å½•ç³»ç»Ÿ...")
        
        try:
            # è·å–ç™»å½•é¡µé¢
            response = self.session.get(self.login_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            form = soup.find('form', {'method': 'post'}) or soup.find('form')
            
            if not form:
                print("âŒ æœªæ‰¾åˆ°ç™»å½•è¡¨å•")
                return False
            
            # æå–è¡¨å•æ•°æ®
            form_data = {}
            inputs = form.find_all('input')
            for input_elem in inputs:
                name = input_elem.get('name')
                value = input_elem.get('value', '')
                if name:
                    form_data[name] = value
            
            # è®¾ç½®ç™»å½•å‡­æ®
            username_field = None
            password_field = None
            
            for input_elem in inputs:
                name = input_elem.get('name', '')
                input_type = input_elem.get('type', 'text')
                
                if 'user' in name.lower():
                    username_field = name
                elif input_type == 'password':
                    password_field = name
            
            if username_field and password_field:
                form_data[username_field] = self.username
                form_data[password_field] = self.password
                
                # å‘é€ç™»å½•è¯·æ±‚
                login_response = self.session.post(
                    self.login_url,
                    data=form_data,
                    timeout=15,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ
                if "Login.aspx" not in login_response.url or "ThinkWater" in login_response.text:
                    print("âœ… ç™»å½•æˆåŠŸï¼")
                    return True
                else:
                    print("âŒ ç™»å½•å¤±è´¥")
                    return False
            else:
                print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·åæˆ–å¯†ç å­—æ®µ")
                return False
                
        except Exception as e:
            print(f"ç™»å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def find_report_page(self):
        """æŸ¥æ‰¾æŠ¥è¡¨é¡µé¢"""
        print("æ­£åœ¨æŸ¥æ‰¾æŠ¥è¡¨é¡µé¢...")
        
        try:
            # è·å–ä¸»é¡µé¢
            main_response = self.session.get(f"{self.base_url}/frmMain.aspx", timeout=10)
            if main_response.status_code == 200:
                soup = BeautifulSoup(main_response.text, 'html.parser')
                
                # æŸ¥æ‰¾æŠ¥è¡¨é“¾æ¥
                links = soup.find_all('a', href=True)
                for link in links:
                    href = link.get('href')
                    text = link.text.strip()
                    
                    if 'æŠ¥è¡¨' in text or 'report' in href.lower():
                        full_url = urljoin(self.base_url, href)
                        print(f"æ‰¾åˆ°æŠ¥è¡¨é“¾æ¥: {text} -> {full_url}")
                        return full_url
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è®¿é—®å·²çŸ¥çš„æŠ¥è¡¨é¡µé¢
            report_url = f"{self.base_url}/reports/FluxRpt.aspx"
            test_response = self.session.get(report_url, timeout=10)
            if test_response.status_code == 200:
                print(f"âœ… ç›´æ¥è®¿é—®æŠ¥è¡¨é¡µé¢æˆåŠŸ: {report_url}")
                return report_url
                
        except Exception as e:
            print(f"æŸ¥æ‰¾æŠ¥è¡¨é¡µé¢å¤±è´¥: {e}")
        
        return None
    
    def try_different_parameters(self, report_url):
        """å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆè·å–æ•°æ®"""
        print("æ­£åœ¨å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ...")
        
        # ä¸åŒçš„æ°´è¡¨IDæ ¼å¼
        meter_variations = [
            "126118100026",
            "126118100030", 
            "126233040233",
            "2190066",
            "2190493",
            "2501200108",
            "2520005",
            "2520006"
        ]
        
        # ä¸åŒçš„å‚æ•°ç»„åˆ
        param_combinations = [
            # ç»„åˆ1ï¼šåŸºæœ¬å‚æ•°
            {
                'rptType': '1',
                'startDate': self.start_date,
                'endDate': self.end_date,
                'user': meter_variations[3],  # ä½¿ç”¨2190066
                'meterType': '',
                'page': '1',
                'rows': '50'
            },
            # ç»„åˆ2ï¼šå®æ—¶æ•°æ®
            {
                'reportType': 'realtime',
                'dateFrom': self.start_date,
                'dateTo': self.end_date,
                'meterId': meter_variations[3],
                'type': 'flux'
            },
            # ç»„åˆ3ï¼šæ—¥æŠ¥è¡¨æ ¼å¼
            {
                'type': 'daily',
                'begin': self.start_date,
                'end': self.end_date,
                'meters': meter_variations[3],
                'format': 'json'
            },
            # ç»„åˆ4ï¼šä½¿ç”¨ä¸åŒæ—¥æœŸæ ¼å¼
            {
                'rptType': '1',
                'startDate': self.start_date.replace('-', '/'),
                'endDate': self.end_date.replace('-', '/'),
                'user': meter_variations[3],
                'statisticsType': '1'
            }
        ]
        
        # å¯èƒ½çš„Ajaxç«¯ç‚¹
        ajax_endpoints = [
            f"{report_url}",
            f"{report_url}?action=getData",
            f"{self.base_url}/reports/GetFluxData.aspx",
            f"{self.base_url}/Handler/GetReportData.ashx",
            f"{self.base_url}/ajax/FluxReport.ashx",
            f"{self.base_url}/reports/ajax/getData.aspx"
        ]
        
        # å°è¯•æ¯ä¸ªå‚æ•°ç»„åˆå’Œç«¯ç‚¹
        for i, params in enumerate(param_combinations):
            print(f"\nğŸ”„ å°è¯•å‚æ•°ç»„åˆ {i+1}: {params}")
            
            for endpoint in ajax_endpoints:
                try:
                    # GETè¯·æ±‚
                    response = self.session.get(endpoint, params=params, timeout=10)
                    if self.check_response_has_data(response, f"GET {endpoint}"):
                        return True
                    
                    # POSTè¯·æ±‚
                    response = self.session.post(endpoint, data=params, timeout=10)
                    if self.check_response_has_data(response, f"POST {endpoint}"):
                        return True
                        
                except Exception as e:
                    continue
        
        return False
    
    def check_login_timeout(self, content):
        """æ£€æŸ¥æ˜¯å¦ç™»å½•è¶…æ—¶"""
        if 'ç™»å½•è¶…æ—¶' in content or 'é‡æ–°ç™»å½•' in content or 'Login.aspx' in content:
            print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•è¶…æ—¶ï¼Œå°è¯•é‡æ–°ç™»å½•...")
            if self.login():
                print("âœ… é‡æ–°ç™»å½•æˆåŠŸ")
                return True
            else:
                print("âŒ é‡æ–°ç™»å½•å¤±è´¥")
                return False
        return False
    
    def check_response_has_data(self, response, request_info):
        """æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æ•°æ®"""
        try:
            if response.status_code != 200:
                return False
            
            content = response.text.strip()
            if not content:
                return False
            
            # æ£€æŸ¥ç™»å½•è¶…æ—¶
            if self.check_login_timeout(content):
                return False  # éœ€è¦é‡è¯•
            
            # æ’é™¤æ— æ„ä¹‰çš„å“åº”
            if (len(content) < 50 or 
                'ç™»å½•è¶…æ—¶' in content or 
                'alert(' in content or
                content == 'null' or
                content == '[]' or
                content == '{}'):
                return False
            
            # æ£€æŸ¥JSONå“åº”
            try:
                if response.headers.get('content-type', '').lower().find('json') >= 0:
                    data = response.json()
                    if data and (isinstance(data, dict) and data.get('rows') or 
                               isinstance(data, list) and len(data) > 0):
                        print(f"âœ… {request_info} - è·å–åˆ°JSONæ•°æ®:")
                        self.display_json_data(data)
                        return True
            except:
                pass
            
            # æ£€æŸ¥HTMLå“åº”
            if '<table' in content or '<tr' in content:
                soup = BeautifulSoup(content, 'html.parser')
                tables = soup.find_all('table')
                
                for table in tables:
                    rows = table.find_all('tr')
                    if len(rows) > 1:  # æœ‰æ•°æ®è¡Œ
                        data_rows = []
                        for row in rows:
                            cells = row.find_all(['td', 'th'])
                            if cells:
                                row_data = [cell.get_text(strip=True) for cell in cells]
                                if any(row_data):  # éç©ºè¡Œ
                                    data_rows.append(row_data)
                        
                        if len(data_rows) > 1:  # é™¤äº†è¡¨å¤´è¿˜æœ‰æ•°æ®
                            print(f"âœ… {request_info} - è·å–åˆ°HTMLè¡¨æ ¼æ•°æ®:")
                            self.display_table_data(data_rows)
                            return True
            
            # æ£€æŸ¥å…¶ä»–æ ¼å¼çš„æœ‰æ„ä¹‰æ•°æ®
            meaningful_keywords = ['æ°´è¡¨', 'meter', 'flux', 'æµé‡', 'ç”¨æ°´é‡', 'æŠ„è¡¨æ—¶é—´', 'æœ€å°å€¼', 'å¹³å‡å€¼']
            if (len(content) > 100 and 
                any(keyword in content.lower() for keyword in meaningful_keywords) and
                'alert(' not in content):
                print(f"âœ… {request_info} - è·å–åˆ°æ–‡æœ¬æ•°æ®:")
                print(content[:500] + "..." if len(content) > 500 else content)
                return True
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å“åº”æ•°æ®å¤±è´¥: {e}")
        
        return False
    
    def display_json_data(self, data):
        """æ˜¾ç¤ºJSONæ•°æ®"""
        try:
            print("\n" + "="*80)
            print("ğŸ“Š æ°´è¡¨æ•°æ®è·å–ç»“æœï¼ˆJSONæ ¼å¼ï¼‰ï¼š")
            print("="*80)
            
            if isinstance(data, dict):
                if 'rows' in data and data['rows']:
                    rows = data['rows']
                    if isinstance(rows[0], dict):
                        headers = list(rows[0].keys())
                        print("è¡¨å¤´:", " | ".join(headers))
                        print("-" * 80)
                        
                        for i, row in enumerate(rows[:10]):  # åªæ˜¾ç¤ºå‰10è¡Œ
                            row_data = [str(row.get(h, '')) for h in headers]
                            print(f"ç¬¬{i+1}è¡Œ:", " | ".join(row_data))
                        
                        if len(rows) > 10:
                            print(f"... è¿˜æœ‰ {len(rows) - 10} è¡Œæ•°æ®")
                
                else:
                    print("å®Œæ•´æ•°æ®:")
                    print(json.dumps(data, indent=2, ensure_ascii=False)[:1000])
            
            elif isinstance(data, list) and data:
                for i, item in enumerate(data[:5]):  # åªæ˜¾ç¤ºå‰5é¡¹
                    print(f"é¡¹ç›® {i+1}:", json.dumps(item, indent=2, ensure_ascii=False))
                
                if len(data) > 5:
                    print(f"... è¿˜æœ‰ {len(data) - 5} é¡¹æ•°æ®")
            
            print("="*80)
            
        except Exception as e:
            print(f"æ˜¾ç¤ºJSONæ•°æ®å¤±è´¥: {e}")
    
    def display_table_data(self, data_rows):
        """æ˜¾ç¤ºè¡¨æ ¼æ•°æ®"""
        try:
            print("\n" + "="*80)
            print("ğŸ“Š æ°´è¡¨æ•°æ®è·å–ç»“æœï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰ï¼š")
            print("="*80)
            
            if data_rows:
                # è¡¨å¤´
                print("è¡¨å¤´:", " | ".join(data_rows[0]))
                print("-" * 80)
                
                # æ•°æ®è¡Œ
                for i, row in enumerate(data_rows[1:11]):  # åªæ˜¾ç¤ºå‰10è¡Œæ•°æ®
                    print(f"ç¬¬{i+1}è¡Œ:", " | ".join(row))
                
                if len(data_rows) > 11:
                    print(f"... è¿˜æœ‰ {len(data_rows) - 11} è¡Œæ•°æ®")
            
            print("="*80)
            
        except Exception as e:
            print(f"æ˜¾ç¤ºè¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
    
    def get_water_data(self):
        """å®Œæ•´çš„æ•°æ®è·å–æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ™ºèƒ½æ°´åŠ¡æ•°æ®è·å–æµç¨‹")
        print("="*60)
        
        try:
            # 1. ç™»å½•
            if not self.login():
                print("âŒ ç™»å½•å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return False
            
            # 2. æŸ¥æ‰¾æŠ¥è¡¨é¡µé¢
            report_url = self.find_report_page()
            if not report_url:
                print("âŒ æœªæ‰¾åˆ°æŠ¥è¡¨é¡µé¢ï¼Œç»ˆæ­¢æµç¨‹")
                return False
            
            # 3. ä½¿ç”¨ä¸åŒå‚æ•°å°è¯•è·å–æ•°æ®
            if self.try_different_parameters(report_url):
                print("\nğŸ‰ æˆåŠŸè·å–åˆ°æ°´åŠ¡æ•°æ®ï¼")
                return True
            else:
                print("\nâŒ å°è¯•äº†æ‰€æœ‰å‚æ•°ç»„åˆï¼Œä»æœªè·å–åˆ°æ•°æ®")
                return False
            
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–æµç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    scraper = SmartWaterDataScraper()
    
    try:
        if scraper.get_water_data():
            print("\nâœ… æ™ºèƒ½æ•°æ®è·å–æˆåŠŸï¼")
        else:
            print("\nâŒ æ™ºèƒ½æ•°æ®è·å–å¤±è´¥ï¼")
            
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()