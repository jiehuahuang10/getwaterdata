#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ°´åŠ¡æ•°æ®è·å–è„šæœ¬ - HTTPè¯·æ±‚ç‰ˆæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨HTTPè¯·æ±‚ç›´æ¥è·å–å¹¿å·å¢åŸè‡ªæ¥æ°´å…¬å¸ThinkWateræ™ºæ…§æ°´ç½‘ç³»ç»Ÿæ•°æ®
"""

import requests
import re
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin, urlparse


class WaterDataHttpScraper:
    def __init__(self):
        """åˆå§‹åŒ–HTTPçˆ¬è™«"""
        self.session = requests.Session()
        
        # ç³»ç»Ÿç™»å½•ä¿¡æ¯
        self.base_url = "http://axwater.dmas.cn"
        self.login_url = f"{self.base_url}/Login.aspx"
        self.username = "13509288500"
        self.password = "288500"
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ç¦ç”¨SSLéªŒè¯è­¦å‘Š
        requests.packages.urllib3.disable_warnings()
    
    def get_login_page(self):
        """è·å–ç™»å½•é¡µé¢ï¼Œæå–å¿…è¦çš„å‚æ•°"""
        print("æ­£åœ¨è·å–ç™»å½•é¡µé¢...")
        
        try:
            response = self.session.get(self.login_url, timeout=10)
            response.raise_for_status()
            
            print(f"ç™»å½•é¡µé¢çŠ¶æ€ç : {response.status_code}")
            print(f"é¡µé¢æ ‡é¢˜: {response.text[:100]}...")
            
            # è§£æHTMLé¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾è¡¨å•
            form = soup.find('form', {'method': 'post'}) or soup.find('form')
            if not form:
                print("âŒ æœªæ‰¾åˆ°ç™»å½•è¡¨å•")
                return None
            
            print("âœ… æ‰¾åˆ°ç™»å½•è¡¨å•")
            
            # æå–è¡¨å•æ•°æ®
            form_data = {}
            
            # æŸ¥æ‰¾æ‰€æœ‰inputå…ƒç´ 
            inputs = form.find_all('input')
            for input_elem in inputs:
                name = input_elem.get('name')
                value = input_elem.get('value', '')
                input_type = input_elem.get('type', 'text')
                
                if name:
                    form_data[name] = value
                    print(f"è¡¨å•å­—æ®µ: {name} = {value} (type: {input_type})")
            
            # æŸ¥æ‰¾ç”¨æˆ·åå’Œå¯†ç å­—æ®µ
            username_field = None
            password_field = None
            
            for input_elem in inputs:
                name = input_elem.get('name', '')
                input_type = input_elem.get('type', 'text')
                
                if 'user' in name.lower() or 'name' in name.lower():
                    username_field = name
                elif input_type == 'password':
                    password_field = name
            
            print(f"ç”¨æˆ·åå­—æ®µ: {username_field}")
            print(f"å¯†ç å­—æ®µ: {password_field}")
            
            return {
                'form_data': form_data,
                'username_field': username_field,
                'password_field': password_field,
                'action': form.get('action', ''),
                'soup': soup
            }
            
        except requests.RequestException as e:
            print(f"è·å–ç™»å½•é¡µé¢å¤±è´¥: {e}")
            return None
    
    def login(self):
        """æ‰§è¡Œç™»å½•"""
        print("å¼€å§‹ç™»å½•ç³»ç»Ÿ...")
        
        # è·å–ç™»å½•é¡µé¢ä¿¡æ¯
        login_info = self.get_login_page()
        if not login_info:
            return False
        
        form_data = login_info['form_data']
        username_field = login_info['username_field']
        password_field = login_info['password_field']
        action = login_info['action']
        
        if not username_field or not password_field:
            print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·åæˆ–å¯†ç å­—æ®µ")
            return False
        
        # è®¾ç½®ç™»å½•å‡­æ®
        form_data[username_field] = self.username
        form_data[password_field] = self.password
        
        print(f"å‡†å¤‡ç™»å½•æ•°æ®:")
        for key, value in form_data.items():
            if 'password' in key.lower():
                print(f"  {key}: {'*' * len(str(value))}")
            else:
                print(f"  {key}: {value}")
        
        # ç¡®å®šç™»å½•URL
        if action:
            login_post_url = urljoin(self.base_url, action)
        else:
            login_post_url = self.login_url
        
        print(f"ç™»å½•POST URL: {login_post_url}")
        
        try:
            # å‘é€ç™»å½•è¯·æ±‚
            print("æ­£åœ¨å‘é€ç™»å½•è¯·æ±‚...")
            response = self.session.post(
                login_post_url,
                data=form_data,
                timeout=15,
                allow_redirects=True
            )
            
            print(f"ç™»å½•å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"æœ€ç»ˆURL: {response.url}")
            
            # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
            if self.check_login_success(response):
                print("âœ… ç™»å½•æˆåŠŸï¼")
                return True
            else:
                print("âŒ ç™»å½•å¤±è´¥")
                # æ‰“å°å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.find('title')
                print(f"é¡µé¢æ ‡é¢˜: {title.text if title else 'æœªçŸ¥'}")
                
                # æŸ¥æ‰¾é”™è¯¯ä¿¡æ¯
                error_elements = soup.find_all(['div', 'span'], class_=re.compile(r'error|alert|warning', re.I))
                for error in error_elements:
                    if error.text.strip():
                        print(f"é”™è¯¯ä¿¡æ¯: {error.text.strip()}")
                
                return False
                
        except requests.RequestException as e:
            print(f"ç™»å½•è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    def check_login_success(self, response):
        """æ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ"""
        # æ£€æŸ¥URLå˜åŒ–
        if "Login.aspx" not in response.url and ("Main" in response.url or "main" in response.url):
            return True
        
        # æ£€æŸ¥é¡µé¢å†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æˆåŠŸç™»å½•çš„æ ‡å¿—
        success_indicators = [
            'ThinkWater',
            'æ™ºæ…§æ°´ç½‘',
            'ç”¨æˆ·',
            'é€€å‡º',
            'æŠ¥è¡¨',
            'frmMain'
        ]
        
        page_text = response.text.lower()
        for indicator in success_indicators:
            if indicator.lower() in page_text:
                print(f"æ‰¾åˆ°ç™»å½•æˆåŠŸæ ‡å¿—: {indicator}")
                return True
        
        # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ç™»å½•é¡µé¢
        if "ç”¨æˆ·å" in response.text and "å¯†ç " in response.text:
            return False
        
        return True
    
    def get_reports_page(self):
        """è·å–æŠ¥è¡¨é¡µé¢"""
        print("æ­£åœ¨è·å–æŠ¥è¡¨é¡µé¢...")
        
        # å¸¸è§çš„æŠ¥è¡¨é¡µé¢URL
        report_urls = [
            f"{self.base_url}/Report/Default.aspx",
            f"{self.base_url}/Reports.aspx",
            f"{self.base_url}/Report.aspx",
            f"{self.base_url}/frmReport.aspx"
        ]
        
        for url in report_urls:
            try:
                print(f"å°è¯•è®¿é—®: {url}")
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    print(f"âœ… æˆåŠŸè®¿é—®æŠ¥è¡¨é¡µé¢: {url}")
                    return response
                else:
                    print(f"âŒ è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except requests.RequestException as e:
                print(f"âŒ è®¿é—® {url} å¤±è´¥: {e}")
                continue
        
        print("âŒ æ— æ³•è®¿é—®æŠ¥è¡¨é¡µé¢ï¼Œå°è¯•æŸ¥æ‰¾é“¾æ¥...")
        return None
    
    def find_report_links(self):
        """æŸ¥æ‰¾ä¸»é¡µé¢ä¸­çš„æŠ¥è¡¨é“¾æ¥"""
        print("æ­£åœ¨æŸ¥æ‰¾æŠ¥è¡¨ç›¸å…³é“¾æ¥...")
        
        try:
            # è·å–ä¸»é¡µé¢
            main_urls = [
                f"{self.base_url}/frmMain.aspx",
                f"{self.base_url}/Main.aspx",
                f"{self.base_url}/Default.aspx"
            ]
            
            for url in main_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾åŒ…å«"æŠ¥è¡¨"çš„é“¾æ¥
                        links = soup.find_all('a', href=True)
                        for link in links:
                            href = link.get('href')
                            text = link.text.strip()
                            
                            if 'æŠ¥è¡¨' in text or 'report' in href.lower():
                                full_url = urljoin(self.base_url, href)
                                print(f"æ‰¾åˆ°æŠ¥è¡¨é“¾æ¥: {text} -> {full_url}")
                                
                                # å°è¯•è®¿é—®è¿™ä¸ªé“¾æ¥
                                try:
                                    report_response = self.session.get(full_url, timeout=10)
                                    if report_response.status_code == 200:
                                        return report_response
                                except:
                                    continue
                        
                        break
                except:
                    continue
                    
        except Exception as e:
            print(f"æŸ¥æ‰¾æŠ¥è¡¨é“¾æ¥å¤±è´¥: {e}")
        
        return None
    
    def get_water_data_api(self):
        """å°è¯•é€šè¿‡APIç›´æ¥è·å–æ•°æ®"""
        print("æ­£åœ¨å°è¯•é€šè¿‡APIè·å–æ•°æ®...")
        
        # å¸¸è§çš„APIç«¯ç‚¹
        api_endpoints = [
            f"{self.base_url}/api/WaterData",
            f"{self.base_url}/Handler/WaterData.ashx",
            f"{self.base_url}/ajax/GetWaterData.aspx",
            f"{self.base_url}/GetData.aspx"
        ]
        
        # ç›®æ ‡æ°´è¡¨ID
        meter_ids = [
            "126118100026",  # è”æ–°å¤§é“DN1200æµé‡è®¡
            "126118100030",  # æ–°å¡˜å¤§é“åŒ»é™¢DN800æµé‡è®¡  
            "126233040233",  # å®è¥¿æ€»è¡¨DN1200
            "2190066",       # ä¸‰æ±Ÿæ–°æ€»è¡¨DN800
            "2190493",       # æ²™åº„æ€»è¡¨
            "2501200108",    # 2501200108
            "2520005",       # å¦‚ä¸°å¤§é“600ç›‘æ§è¡¨
            "2520006"        # ä¸‰æ£’æ¡¥600ç›‘æ§è¡¨
        ]
        
        # æ„é€ è¯·æ±‚å‚æ•°
        params = {
            'reportType': 'daily',
            'startDate': '2025-07-24',
            'endDate': '2025-07-31',
            'meterIds': ','.join(meter_ids)
        }
        
        for endpoint in api_endpoints:
            try:
                print(f"å°è¯•APIç«¯ç‚¹: {endpoint}")
                
                # GETè¯·æ±‚
                response = self.session.get(endpoint, params=params, timeout=10)
                if response.status_code == 200:
                    print(f"âœ… APIè°ƒç”¨æˆåŠŸ: {endpoint}")
                    return self.parse_api_response(response)
                
                # POSTè¯·æ±‚
                response = self.session.post(endpoint, data=params, timeout=10)
                if response.status_code == 200:
                    print(f"âœ… APIè°ƒç”¨æˆåŠŸ: {endpoint}")
                    return self.parse_api_response(response)
                    
            except Exception as e:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥ {endpoint}: {e}")
                continue
        
        return False
    
    def parse_api_response(self, response):
        """è§£æAPIå“åº”"""
        try:
            # å°è¯•è§£æJSON
            if 'json' in response.headers.get('content-type', '').lower():
                data = response.json()
                print("âœ… è·å–åˆ°JSONæ•°æ®:")
                print(json.dumps(data, indent=2, ensure_ascii=False))
                return True
            
            # å°è¯•è§£æHTMLè¡¨æ ¼
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                print("âœ… æ‰¾åˆ°æ•°æ®è¡¨æ ¼:")
                for i, table in enumerate(tables):
                    print(f"\nè¡¨æ ¼ {i+1}:")
                    rows = table.find_all('tr')
                    for j, row in enumerate(rows):
                        cells = row.find_all(['td', 'th'])
                        if cells:
                            row_data = [cell.get_text(strip=True) for cell in cells]
                            print(f"  è¡Œ {j+1}: {' | '.join(row_data)}")
                return True
            
            # æ‰“å°åŸå§‹æ–‡æœ¬
            if response.text.strip():
                print("å“åº”å†…å®¹:")
                print(response.text[:1000])
                return True
                
        except Exception as e:
            print(f"è§£æå“åº”å¤±è´¥: {e}")
        
        return False
    
    def get_water_data(self):
        """å®Œæ•´çš„æ•°æ®è·å–æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹HTTPè¯·æ±‚æ–¹å¼çš„æ°´åŠ¡æ•°æ®è·å–æµç¨‹")
        print("="*60)
        
        try:
            # 1. ç™»å½•
            if not self.login():
                print("âŒ ç™»å½•å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return False
            
            # 2. å°è¯•é€šè¿‡APIè·å–æ•°æ®
            if self.get_water_data_api():
                print("âœ… é€šè¿‡APIæˆåŠŸè·å–æ•°æ®")
                return True
            
            # 3. å°è¯•è®¿é—®æŠ¥è¡¨é¡µé¢
            report_response = self.get_reports_page()
            if not report_response:
                report_response = self.find_report_links()
            
            if report_response:
                print("âœ… æˆåŠŸè®¿é—®æŠ¥è¡¨é¡µé¢")
                
                # å…ˆå°è¯•ä»å½“å‰é¡µé¢æå–æ•°æ®
                if self.extract_data_from_html(report_response.text, report_response.url):
                    return True
                
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œå°è¯•æäº¤æŸ¥è¯¢è¯·æ±‚
                print("å½“å‰é¡µé¢æ— æ•°æ®ï¼Œå°è¯•æäº¤æŸ¥è¯¢è¯·æ±‚...")
                soup = BeautifulSoup(report_response.text, 'html.parser')
                query_response = self.submit_report_query(report_response.url, soup)
                
                if query_response:
                    return self.extract_data_from_html(query_response.text, query_response.url)
                else:
                    print("âŒ æŸ¥è¯¢è¯·æ±‚å¤±è´¥")
                    return False
            else:
                print("âŒ æ— æ³•è®¿é—®æŠ¥è¡¨é¡µé¢")
                return False
            
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–æµç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def analyze_report_page(self, html_content):
        """åˆ†ææŠ¥è¡¨é¡µé¢ç»“æ„"""
        print("æ­£åœ¨åˆ†ææŠ¥è¡¨é¡µé¢ç»“æ„...")
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ä¿å­˜é¡µé¢å†…å®¹åˆ°æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
            with open('report_page.html', 'w', encoding='utf-8') as f:
                f.write(html_content)
            print("âœ… æŠ¥è¡¨é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° report_page.html")
            
            # æŸ¥æ‰¾è¡¨å•
            forms = soup.find_all('form')
            print(f"æ‰¾åˆ° {len(forms)} ä¸ªè¡¨å•")
            
            for i, form in enumerate(forms):
                print(f"\nè¡¨å• {i+1}:")
                print(f"  Action: {form.get('action', 'æœªè®¾ç½®')}")
                print(f"  Method: {form.get('method', 'æœªè®¾ç½®')}")
                
                # æŸ¥æ‰¾è¡¨å•ä¸­çš„è¾“å…¥å­—æ®µ
                inputs = form.find_all(['input', 'select', 'textarea'])
                for input_elem in inputs:
                    name = input_elem.get('name', '')
                    input_type = input_elem.get('type', input_elem.name)
                    value = input_elem.get('value', '')
                    
                    if name:
                        print(f"    {name}: {input_type} = {value}")
            
            # æŸ¥æ‰¾ä¸‹æ‹‰æ¡†é€‰é¡¹
            selects = soup.find_all('select')
            for select in selects:
                select_name = select.get('name', 'æœªçŸ¥')
                print(f"\nä¸‹æ‹‰æ¡† {select_name}:")
                options = select.find_all('option')
                for option in options:
                    value = option.get('value', '')
                    text = option.get_text(strip=True)
                    print(f"  {value}: {text}")
            
            return soup
            
        except Exception as e:
            print(f"åˆ†ææŠ¥è¡¨é¡µé¢å¤±è´¥: {e}")
            return None
    
    def get_ajax_data(self, report_url):
        """å°è¯•é€šè¿‡Ajaxè·å–æ•°æ®"""
        print("æ­£åœ¨å°è¯•é€šè¿‡Ajaxè·å–æ•°æ®...")
        
        # å¸¸è§çš„Ajaxç«¯ç‚¹
        ajax_endpoints = [
            f"{report_url}?action=getData",
            f"{report_url}?method=getdata",
            f"{self.base_url}/reports/GetFluxData.aspx",
            f"{self.base_url}/Handler/FluxData.ashx",
            f"{self.base_url}/reports/ajax/GetData.aspx"
        ]
        
        # æ„é€ Ajaxè¯·æ±‚å‚æ•°
        ajax_params = {
            'rptType': '1',  # æ—¥æŠ¥è¡¨
            'meterType': '',  # æ‰€æœ‰ç±»å‹
            'startDate': '2025-07-24',
            'endDate': '2025-07-31',
            'user': '126118100026,126118100030,126233040233,2190066,2190493,2501200108,2520005,2520006',
            'page': '1',
            'rows': '100'
        }
        
        for endpoint in ajax_endpoints:
            try:
                print(f"å°è¯•Ajaxç«¯ç‚¹: {endpoint}")
                
                # GETè¯·æ±‚
                response = self.session.get(endpoint, params=ajax_params, timeout=10)
                if response.status_code == 200 and response.text.strip():
                    print(f"âœ… Ajax GETæˆåŠŸ: {endpoint}")
                    return self.parse_ajax_response(response)
                
                # POSTè¯·æ±‚
                response = self.session.post(endpoint, data=ajax_params, timeout=10)
                if response.status_code == 200 and response.text.strip():
                    print(f"âœ… Ajax POSTæˆåŠŸ: {endpoint}")
                    return self.parse_ajax_response(response)
                    
            except Exception as e:
                print(f"âŒ Ajaxè¯·æ±‚å¤±è´¥ {endpoint}: {e}")
                continue
        
        return False
    
    def parse_ajax_response(self, response):
        """è§£æAjaxå“åº”"""
        try:
            content_type = response.headers.get('content-type', '').lower()
            
            # JSONå“åº”
            if 'json' in content_type:
                data = response.json()
                print("âœ… è·å–åˆ°JSONæ•°æ®:")
                self.format_json_data(data)
                return True
            
            # HTMLè¡¨æ ¼å“åº”
            elif 'html' in content_type:
                soup = BeautifulSoup(response.text, 'html.parser')
                tables = soup.find_all('table')
                if tables:
                    print("âœ… è·å–åˆ°HTMLè¡¨æ ¼æ•°æ®:")
                    return self.extract_data_from_html(response.text)
            
            # çº¯æ–‡æœ¬å“åº”
            else:
                text = response.text.strip()
                if text:
                    print("âœ… è·å–åˆ°æ–‡æœ¬æ•°æ®:")
                    print(text[:1000])
                    return True
                    
        except Exception as e:
            print(f"è§£æAjaxå“åº”å¤±è´¥: {e}")
        
        return False
    
    def format_json_data(self, data):
        """æ ¼å¼åŒ–JSONæ•°æ®"""
        try:
            print("\n" + "="*80)
            print("ğŸ“Š æ°´è¡¨æ•°æ®è·å–ç»“æœï¼ˆJSONæ ¼å¼ï¼‰ï¼š")
            print("="*80)
            
            if isinstance(data, dict):
                if 'rows' in data:
                    rows = data['rows']
                    if rows:
                        # æ‰“å°è¡¨å¤´
                        if isinstance(rows[0], dict):
                            headers = list(rows[0].keys())
                            print("è¡¨å¤´:", " | ".join(headers))
                            print("-" * 80)
                            
                            # æ‰“å°æ•°æ®è¡Œ
                            for i, row in enumerate(rows):
                                row_data = [str(row.get(h, '')) for h in headers]
                                print(f"ç¬¬{i+1}è¡Œ:", " | ".join(row_data))
                
                elif 'data' in data:
                    print("æ•°æ®å†…å®¹:", json.dumps(data['data'], indent=2, ensure_ascii=False))
                
                else:
                    print("å®Œæ•´æ•°æ®:", json.dumps(data, indent=2, ensure_ascii=False))
            
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    print(f"é¡¹ç›® {i+1}:", json.dumps(item, indent=2, ensure_ascii=False))
            
            print("="*80)
            
        except Exception as e:
            print(f"æ ¼å¼åŒ–JSONæ•°æ®å¤±è´¥: {e}")
    
    def submit_report_query(self, report_url, soup):
        """æäº¤æŠ¥è¡¨æŸ¥è¯¢è¯·æ±‚"""
        print("æ­£åœ¨æäº¤æŠ¥è¡¨æŸ¥è¯¢è¯·æ±‚...")
        
        # é¦–å…ˆå°è¯•Ajaxæ–¹å¼è·å–æ•°æ®
        if self.get_ajax_data(report_url):
            return True
        
        try:
            # æŸ¥æ‰¾è¡¨å•
            form = soup.find('form', {'method': 'post'}) or soup.find('form')
            if not form:
                print("âŒ æœªæ‰¾åˆ°æŸ¥è¯¢è¡¨å•")
                return None
            
            # æ„å»ºè¡¨å•æ•°æ®
            form_data = {}
            
            # è·å–æ‰€æœ‰éšè—å­—æ®µ
            hidden_inputs = form.find_all('input', {'type': 'hidden'})
            for hidden in hidden_inputs:
                name = hidden.get('name')
                value = hidden.get('value', '')
                if name:
                    form_data[name] = value
            
            # æ·»åŠ æŸ¥è¯¢å‚æ•°ï¼ˆåŸºäºHTMLåˆ†æï¼‰
            form_data.update({
                'rptType': '1',  # æ—¥æŠ¥è¡¨
                'startDate': '2025-07-24',
                'endDate': '2025-07-31',
                'user': '126118100026',  # å…ˆæµ‹è¯•ä¸€ä¸ªæ°´è¡¨
                'meterType': '',  # æ‰€æœ‰ç±»å‹
                'search': 'æŸ¥è¯¢'
            })
            
            print("æŸ¥è¯¢è¡¨å•æ•°æ®:")
            for key, value in form_data.items():
                print(f"  {key}: {value}")
            
            # å‘é€æŸ¥è¯¢è¯·æ±‚
            query_url = urljoin(report_url, form.get('action', ''))
            print(f"æŸ¥è¯¢URL: {query_url}")
            
            response = self.session.post(query_url, data=form_data, timeout=15)
            print(f"æŸ¥è¯¢å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                return response
            else:
                print("âŒ æŸ¥è¯¢è¯·æ±‚å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"æäº¤æŸ¥è¯¢è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def extract_data_from_html(self, html_content, page_url=None):
        """ä»HTMLå†…å®¹ä¸­æå–æ•°æ®"""
        print("æ­£åœ¨ä»HTMLä¸­æå–æ•°æ®...")
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # åˆ†æé¡µé¢ç»“æ„
            analyzed_soup = self.analyze_report_page(html_content)
            if not analyzed_soup:
                return False
            
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            tables = soup.find_all('table')
            print(f"æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            
            if not tables:
                print("âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼ï¼Œå¯èƒ½éœ€è¦æäº¤æŸ¥è¯¢è¯·æ±‚")
                return False
            
            # åˆ†ææ¯ä¸ªè¡¨æ ¼
            has_data = False
            for i, table in enumerate(tables):
                rows = table.find_all('tr')
                if len(rows) > 1:  # æœ‰æ•°æ®çš„è¡¨æ ¼
                    print(f"\nğŸ“Š è¡¨æ ¼ {i+1} åŒ…å« {len(rows)} è¡Œæ•°æ®:")
                    print("="*80)
                    
                    for j, row in enumerate(rows):
                        cells = row.find_all(['td', 'th'])
                        if cells:
                            row_data = [cell.get_text(strip=True) for cell in cells]
                            if any(row_data):  # åªæ‰“å°éç©ºè¡Œ
                                has_data = True
                                if j == 0:
                                    print("è¡¨å¤´:", " | ".join(row_data))
                                    print("-" * 80)
                                else:
                                    print(f"ç¬¬{j}è¡Œ:", " | ".join(row_data))
                    
                    print("="*80)
            
            if has_data:
                print("\nâœ… æ•°æ®æå–å®Œæˆ")
                return True
            else:
                print("âš ï¸  æ‰€æœ‰è¡¨æ ¼å‡ä¸ºç©º")
                return False
            
        except Exception as e:
            print(f"æ•°æ®æå–å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    scraper = WaterDataHttpScraper()
    
    try:
        # æ‰§è¡Œå®Œæ•´çš„æ•°æ®è·å–æµç¨‹
        if scraper.get_water_data():
            print("\nğŸ‰ HTTPæ–¹å¼æ•°æ®è·å–æˆåŠŸï¼")
        else:
            print("\nâŒ HTTPæ–¹å¼æ•°æ®è·å–å¤±è´¥ï¼")
            
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()