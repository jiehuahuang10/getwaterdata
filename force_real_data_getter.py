#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåˆ¶è·å–çœŸå®æ•°æ® - å¿…é¡»æˆåŠŸè·å–æŒ‡å®šæ—¥æœŸçš„çœŸå®æ°´è¡¨æ•°æ®
"""

import requests
import json
import hashlib
import time
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import random

def md5_hash(text):
    """è®¡ç®—MD5å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def force_get_real_data(target_date):
    """å¼ºåˆ¶è·å–æŒ‡å®šæ—¥æœŸçš„çœŸå®æ•°æ® - å¤šç§æ–¹æ³•å°è¯•ç›´åˆ°æˆåŠŸ"""
    
    print(f"ğŸ¯ å¼ºåˆ¶è·å– {target_date} çš„çœŸå®æ•°æ®")
    print("ğŸ’ª ä½¿ç”¨å¤šç§ç­–ç•¥ç¡®ä¿æˆåŠŸè·å–")
    
    strategies = [
        "single_day_query",
        "range_query_small", 
        "range_query_medium",
        "range_query_large",
        "different_api_params",
        "retry_with_delay",
        "browser_simulation",
        "direct_page_scraping"
    ]
    
    for strategy_name in strategies:
        print(f"\nğŸ”„ å°è¯•ç­–ç•¥: {strategy_name}")
        result = try_strategy(target_date, strategy_name)
        
        if result and result.get('data') and result['data'].get('rows'):
            print(f"âœ… ç­–ç•¥ {strategy_name} æˆåŠŸï¼è·å–åˆ° {len(result['data']['rows'])} ä¸ªæ°´è¡¨æ•°æ®")
            return result
        else:
            print(f"âŒ ç­–ç•¥ {strategy_name} å¤±è´¥")
    
    print("ğŸš¨ æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†ï¼Œä½†æˆ‘ä»¬å¿…é¡»è·å–çœŸå®æ•°æ®ï¼")
    print("ğŸ”§ ä½¿ç”¨æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ...")
    
    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å·²çŸ¥æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶ï¼Œä½†æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
    return get_closest_real_data(target_date)

def try_strategy(target_date, strategy):
    """å°è¯•ä¸åŒçš„ç­–ç•¥è·å–æ•°æ®"""
    
    session = requests.Session()
    
    # ç™»å½•
    if not login_to_system(session):
        return None
    
    if strategy == "single_day_query":
        return single_day_api_call(session, target_date)
    
    elif strategy == "range_query_small":
        # å°èŒƒå›´æŸ¥è¯¢ï¼šç›®æ ‡æ—¥æœŸå‰å1å¤©
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        start_date = (target_dt - timedelta(days=1)).strftime('%Y-%m-%d')
        end_date = (target_dt + timedelta(days=1)).strftime('%Y-%m-%d')
        return range_api_call(session, start_date, end_date, target_date)
    
    elif strategy == "range_query_medium":
        # ä¸­ç­‰èŒƒå›´æŸ¥è¯¢ï¼šç›®æ ‡æ—¥æœŸå‰å3å¤©
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        start_date = (target_dt - timedelta(days=3)).strftime('%Y-%m-%d')
        end_date = (target_dt + timedelta(days=3)).strftime('%Y-%m-%d')
        return range_api_call(session, start_date, end_date, target_date)
    
    elif strategy == "range_query_large":
        # å¤§èŒƒå›´æŸ¥è¯¢ï¼šç›®æ ‡æ—¥æœŸå‰å7å¤©
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        start_date = (target_dt - timedelta(days=7)).strftime('%Y-%m-%d')
        end_date = (target_dt + timedelta(days=7)).strftime('%Y-%m-%d')
        return range_api_call(session, start_date, end_date, target_date)
    
    elif strategy == "different_api_params":
        # å°è¯•ä¸åŒçš„APIå‚æ•°ç»„åˆ
        return try_different_params(session, target_date)
    
    elif strategy == "retry_with_delay":
        # é‡è¯•ç­–ç•¥ï¼šå¤šæ¬¡å°è¯•ï¼Œæ¯æ¬¡é—´éš”éšæœºå»¶è¿Ÿ
        return retry_with_random_delay(session, target_date)
    
    elif strategy == "browser_simulation":
        # æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
        return browser_like_request(session, target_date)
    
    elif strategy == "direct_page_scraping":
        # ç›´æ¥æŠ“å–ç½‘é¡µæ•°æ®
        return direct_page_scraping(session, target_date)
    
    return None

def login_to_system(session):
    """ç™»å½•åˆ°æ°´åŠ¡ç³»ç»Ÿ"""
    try:
        login_url = "http://axwater.dmas.cn/login.aspx"
        login_page = session.get(login_url)
        
        soup = BeautifulSoup(login_page.text, 'html.parser')
        form = soup.find('form')
        
        form_data = {}
        if form:
            for input_elem in form.find_all('input'):
                name = input_elem.get('name')
                value = input_elem.get('value', '')
                if name:
                    form_data[name] = value
        
        username = "13509288500"
        password = "288500"
        form_data['user'] = username
        form_data['pwd'] = md5_hash(password)
        
        login_response = session.post(login_url, data=form_data)
        
        if 'window.location' in login_response.text:
            print("âœ… ç™»å½•æˆåŠŸ")
            return True
        else:
            print("âŒ ç™»å½•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
        return False

def single_day_api_call(session, target_date):
    """å•æ—¥APIè°ƒç”¨"""
    try:
        report_url = "http://axwater.dmas.cn/reports/FluxRpt.aspx"
        report_response = session.get(report_url)
        
        if report_response.status_code != 200:
            return None
        
        # æ°´è¡¨ID
        meter_ids = [
            '1261181000263', '1261181000300', '1262330402331', '2190066',
            '2190493', '2501200108', '2520005', '2520006'
        ]
        
        formatted_node_ids = "'" + "','".join(meter_ids) + "'"
        
        # è·å–ASP.NETçŠ¶æ€
        report_soup = BeautifulSoup(report_response.text, 'html.parser')
        viewstate = report_soup.find('input', {'name': '__VIEWSTATE'})
        eventvalidation = report_soup.find('input', {'name': '__EVENTVALIDATION'})
        
        api_url = "http://axwater.dmas.cn/reports/ashx/getRptWaterYield.ashx"
        api_params = {
            'nodeId': formatted_node_ids,
            'startDate': target_date,
            'endDate': target_date,
            'rptType': 'day'
        }
        
        if viewstate:
            api_params['__VIEWSTATE'] = viewstate.get('value', '')
        if eventvalidation:
            api_params['__EVENTVALIDATION'] = eventvalidation.get('value', '')
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': report_url
        }
        
        api_response = session.post(api_url, data=api_params, headers=headers)
        
        print(f"ğŸ“¡ APIå“åº”: {api_response.status_code}, é•¿åº¦: {len(api_response.text)}")
        
        if api_response.status_code == 200 and len(api_response.text.strip()) > 0:
            try:
                json_data = json.loads(api_response.text)
                if 'rows' in json_data and len(json_data['rows']) > 0:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸçš„æ•°æ®
                    for row in json_data['rows']:
                        if target_date in row and isinstance(row[target_date], (int, float)):
                            return {
                                'success': True,
                                'data': json_data,
                                'source': 'force_real_data_single_day'
                            }
                return None
            except json.JSONDecodeError:
                return None
        
        return None
        
    except Exception as e:
        print(f"âŒ å•æ—¥æŸ¥è¯¢å¼‚å¸¸: {e}")
        return None

def range_api_call(session, start_date, end_date, target_date):
    """èŒƒå›´APIè°ƒç”¨"""
    try:
        print(f"ğŸ“… èŒƒå›´æŸ¥è¯¢: {start_date} ~ {end_date}")
        
        report_url = "http://axwater.dmas.cn/reports/FluxRpt.aspx"
        report_response = session.get(report_url)
        
        meter_ids = [
            '1261181000263', '1261181000300', '1262330402331', '2190066',
            '2190493', '2501200108', '2520005', '2520006'
        ]
        
        formatted_node_ids = "'" + "','".join(meter_ids) + "'"
        
        report_soup = BeautifulSoup(report_response.text, 'html.parser')
        viewstate = report_soup.find('input', {'name': '__VIEWSTATE'})
        eventvalidation = report_soup.find('input', {'name': '__EVENTVALIDATION'})
        
        api_url = "http://axwater.dmas.cn/reports/ashx/getRptWaterYield.ashx"
        api_params = {
            'nodeId': formatted_node_ids,
            'startDate': start_date,
            'endDate': end_date,
            'rptType': 'day'
        }
        
        if viewstate:
            api_params['__VIEWSTATE'] = viewstate.get('value', '')
        if eventvalidation:
            api_params['__EVENTVALIDATION'] = eventvalidation.get('value', '')
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': report_url
        }
        
        api_response = session.post(api_url, data=api_params, headers=headers)
        
        if api_response.status_code == 200 and len(api_response.text.strip()) > 0:
            try:
                json_data = json.loads(api_response.text)
                if 'rows' in json_data and len(json_data['rows']) > 0:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸ
                    for row in json_data['rows']:
                        if target_date in row and isinstance(row[target_date], (int, float)):
                            print(f"âœ… åœ¨èŒƒå›´æ•°æ®ä¸­æ‰¾åˆ° {target_date} çš„çœŸå®æ•°æ®ï¼")
                            return {
                                'success': True,
                                'data': json_data,
                                'source': f'force_real_data_range_{start_date}_{end_date}'
                            }
                return None
            except json.JSONDecodeError:
                return None
        
        return None
        
    except Exception as e:
        print(f"âŒ èŒƒå›´æŸ¥è¯¢å¼‚å¸¸: {e}")
        return None

def try_different_params(session, target_date):
    """å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ"""
    print("ğŸ”§ å°è¯•ä¸åŒçš„APIå‚æ•°ç»„åˆ...")
    
    # ä¸åŒçš„rptType
    rpt_types = ['day', 'hour', 'month']
    
    for rpt_type in rpt_types:
        print(f"  ğŸ”„ å°è¯• rptType: {rpt_type}")
        result = single_day_with_rpt_type(session, target_date, rpt_type)
        if result:
            return result
    
    return None

def single_day_with_rpt_type(session, target_date, rpt_type):
    """ä½¿ç”¨æŒ‡å®šçš„rptTypeè¿›è¡Œå•æ—¥æŸ¥è¯¢"""
    try:
        report_url = "http://axwater.dmas.cn/reports/FluxRpt.aspx"
        report_response = session.get(report_url)
        
        meter_ids = [
            '1261181000263', '1261181000300', '1262330402331', '2190066',
            '2190493', '2501200108', '2520005', '2520006'
        ]
        
        formatted_node_ids = "'" + "','".join(meter_ids) + "'"
        
        api_url = "http://axwater.dmas.cn/reports/ashx/getRptWaterYield.ashx"
        api_params = {
            'nodeId': formatted_node_ids,
            'startDate': target_date,
            'endDate': target_date,
            'rptType': rpt_type
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        }
        
        api_response = session.post(api_url, data=api_params, headers=headers)
        
        if api_response.status_code == 200 and len(api_response.text.strip()) > 0:
            try:
                json_data = json.loads(api_response.text)
                if 'rows' in json_data and len(json_data['rows']) > 0:
                    return {
                        'success': True,
                        'data': json_data,
                        'source': f'force_real_data_rptType_{rpt_type}'
                    }
            except json.JSONDecodeError:
                pass
        
        return None
        
    except Exception:
        return None

def retry_with_random_delay(session, target_date):
    """é‡è¯•ç­–ç•¥ï¼šå¤šæ¬¡å°è¯•ï¼Œéšæœºå»¶è¿Ÿ"""
    print("â±ï¸ ä½¿ç”¨é‡è¯•ç­–ç•¥...")
    
    for attempt in range(5):  # å°è¯•5æ¬¡
        delay = random.uniform(1, 3)  # éšæœºå»¶è¿Ÿ1-3ç§’
        print(f"  ğŸ”„ ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œå»¶è¿Ÿ {delay:.1f} ç§’...")
        time.sleep(delay)
        
        result = single_day_api_call(session, target_date)
        if result:
            return result
    
    return None

def browser_like_request(session, target_date):
    """æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º"""
    print("ğŸŒ æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º...")
    
    try:
        # å…ˆè®¿é—®ä¸»é¡µé¢
        main_url = "http://axwater.dmas.cn/frmMain.aspx"
        session.get(main_url)
        time.sleep(1)
        
        # å†è®¿é—®æŠ¥è¡¨é¡µé¢
        report_url = "http://axwater.dmas.cn/reports/FluxRpt.aspx"
        session.get(report_url)
        time.sleep(1)
        
        # ç„¶åè¿›è¡ŒAPIè°ƒç”¨
        return single_day_api_call(session, target_date)
        
    except Exception:
        return None

def direct_page_scraping(session, target_date):
    """ç›´æ¥æŠ“å–ç½‘é¡µè¡¨æ ¼æ•°æ®"""
    print("ğŸ•·ï¸ ç›´æ¥æŠ“å–ç½‘é¡µè¡¨æ ¼æ•°æ®...")
    
    try:
        # æ„å»ºæŠ¥è¡¨é¡µé¢URLï¼Œå¸¦æŸ¥è¯¢å‚æ•°
        report_url = "http://axwater.dmas.cn/reports/FluxRpt.aspx"
        
        # å…ˆè®¿é—®æŠ¥è¡¨é¡µé¢
        report_response = session.get(report_url)
        if report_response.status_code != 200:
            return None
        
        # è§£æé¡µé¢ï¼Œå¯»æ‰¾è¡¨å•
        soup = BeautifulSoup(report_response.text, 'html.parser')
        
        # æŸ¥æ‰¾è¡¨å•å…ƒç´ 
        form = soup.find('form')
        if not form:
            return None
        
        # è·å–æ‰€æœ‰inputå­—æ®µ
        form_data = {}
        for input_elem in form.find_all('input'):
            name = input_elem.get('name')
            value = input_elem.get('value', '')
            if name:
                form_data[name] = value
        
        # è®¾ç½®æŸ¥è¯¢å‚æ•°
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„è¡¨å•å­—æ®µæ¥è®¾ç½®
        # å¯èƒ½çš„å­—æ®µåï¼šstartDate, endDate, nodeIdsç­‰
        
        # å°è¯•ä¸åŒçš„è¡¨å•å­—æ®µç»„åˆ
        possible_date_fields = ['startDate', 'start_date', 'dateFrom', 'beginDate']
        possible_end_fields = ['endDate', 'end_date', 'dateTo', 'endDate']
        
        for start_field in possible_date_fields:
            if start_field in form_data:
                form_data[start_field] = target_date
                break
        
        for end_field in possible_end_fields:
            if end_field in form_data:
                form_data[end_field] = target_date
                break
        
        # æäº¤è¡¨å•
        form_response = session.post(report_url, data=form_data)
        
        if form_response.status_code == 200:
            # è§£æè¿”å›çš„é¡µé¢ï¼ŒæŸ¥æ‰¾æ•°æ®è¡¨æ ¼
            result_soup = BeautifulSoup(form_response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ•°æ®è¡¨æ ¼
            tables = result_soup.find_all('table')
            
            for table in tables:
                # æŸ¥æ‰¾åŒ…å«æ°´è¡¨æ•°æ®çš„è¡¨æ ¼
                rows = table.find_all('tr')
                if len(rows) > 1:  # è‡³å°‘æœ‰è¡¨å¤´å’Œæ•°æ®è¡Œ
                    # å°è¯•è§£æè¡¨æ ¼æ•°æ®
                    table_data = parse_table_data(table, target_date)
                    if table_data:
                        return {
                            'success': True,
                            'data': table_data,
                            'source': 'direct_page_scraping'
                        }
        
        return None
        
    except Exception as e:
        print(f"âŒ ç½‘é¡µæŠ“å–å¼‚å¸¸: {e}")
        return None

def parse_table_data(table, target_date):
    """è§£æè¡¨æ ¼æ•°æ®"""
    try:
        rows = table.find_all('tr')
        if len(rows) < 2:
            return None
        
        # è§£æè¡¨å¤´
        header_row = rows[0]
        headers = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]
        
        # æŸ¥æ‰¾æ—¥æœŸåˆ—
        date_col_index = -1
        for i, header in enumerate(headers):
            if target_date in header or 'æ—¥æœŸ' in header:
                date_col_index = i
                break
        
        if date_col_index == -1:
            return None
        
        # è§£ææ•°æ®è¡Œ
        data_rows = []
        for row in rows[1:]:
            cells = row.find_all(['td', 'th'])
            if len(cells) > date_col_index:
                row_data = {}
                for i, cell in enumerate(cells):
                    if i < len(headers):
                        row_data[headers[i]] = cell.get_text().strip()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸçš„æ•°æ®
                if target_date in str(row_data):
                    data_rows.append(row_data)
        
        if data_rows:
            return {
                'total': len(data_rows),
                'rows': data_rows
            }
        
        return None
        
    except Exception:
        return None

def get_closest_real_data(target_date):
    """è·å–æœ€æ¥è¿‘çš„çœŸå®æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
    print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šè·å–æœ€æ¥è¿‘çš„çœŸå®æ•°æ®...")
    
    try:
        import glob
        import os
        data_files = (glob.glob("*COMPLETE_8_METERS*.json") + 
                     glob.glob("WEB_COMPLETE*.json"))
        
        if not data_files:
            return None
        
        latest_file = max(data_files, key=lambda x: os.path.getmtime(x))
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {latest_file}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡æ—¥æœŸçš„æ•°æ®
        if 'data' in data and 'rows' in data['data']:
            for row in data['data']['rows']:
                if target_date in row and isinstance(row[target_date], (int, float)):
                    print(f"ğŸ‰ åœ¨å¤‡ç”¨æ•°æ®ä¸­æ‰¾åˆ° {target_date} çš„çœŸå®æ•°æ®ï¼")
                    return {
                        'success': True,
                        'data': data['data'],
                        'source': f'backup_real_data_from_{latest_file}'
                    }
        
        print(f"âš ï¸ å¤‡ç”¨æ•°æ®ä¸­æ²¡æœ‰ {target_date} çš„æ•°æ®")
        return None
        
    except Exception as e:
        print(f"âŒ å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    # å…ˆæµ‹è¯•ä¸€ä¸ªæœ€è¿‘çš„æ—¥æœŸï¼ˆæ›´å¯èƒ½æœ‰æ•°æ®ï¼‰
    test_date = "2025-08-15"  # æ”¹ä¸º8æœˆ15æ—¥æµ‹è¯•
    print(f"ğŸš€ å¼ºåˆ¶è·å–çœŸå®æ•°æ®æµ‹è¯•: {test_date}")
    print("=" * 60)
    
    result = force_get_real_data(test_date)
    
    if result and result.get('success'):
        print(f"\nğŸ‰ æˆåŠŸè·å– {test_date} çš„çœŸå®æ•°æ®ï¼")
        print(f"ğŸ“Š æ•°æ®æ¥æº: {result.get('source', 'unknown')}")
        
        if 'data' in result and 'rows' in result['data']:
            print(f"ğŸ“ˆ åŒ…å« {len(result['data']['rows'])} ä¸ªæ°´è¡¨çš„æ•°æ®")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ°´è¡¨çš„ç¤ºä¾‹æ•°æ®
            first_row = result['data']['rows'][0]
            if test_date in first_row:
                print(f"ğŸ’§ ç¤ºä¾‹æ•°æ® ({first_row.get('Name', 'Unknown')}): {first_row[test_date]}")
    else:
        print(f"\nâŒ æ— æ³•è·å– {test_date} çš„çœŸå®æ•°æ®")
    
    print("=" * 60)
